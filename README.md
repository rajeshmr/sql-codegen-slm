# SQL Code Generator - Fine-tuned Small Language Model

A fine-tuned Mistral-7B model that generates PostgreSQL queries from natural language, trained on the Spider dataset.

## Project Status

ğŸš§ **In Progress - Module 1.4: PostgreSQL Conversion**

## Quick Start

Setup instructions coming soon.

## Project Structure

```
sql-codegen-slm/
â”œâ”€â”€ data/                   # Dataset storage
â”‚   â”œâ”€â”€ raw/               # Downloaded Spider dataset
â”‚   â”œâ”€â”€ processed/         # Formatted training data
â”‚   â””â”€â”€ demo/              # Example schemas
â”œâ”€â”€ training/              # Model training
â”‚   â”œâ”€â”€ configs/           # Training configuration files
â”‚   â”œâ”€â”€ logs/              # Training logs and metrics
â”‚   â””â”€â”€ models/            # Saved model checkpoints
â”œâ”€â”€ backend/               # FastAPI application
â”‚   â””â”€â”€ app/               # API implementation
â”œâ”€â”€ frontend/              # Next.js application
â”œâ”€â”€ deployment/            # Deployment configurations
â”‚   â”œâ”€â”€ backend/           # Dockerfile, Cloud Run configs
â”‚   â””â”€â”€ frontend/          # Frontend deployment configs
â”œâ”€â”€ tests/                 # Test suites
â”‚   â”œâ”€â”€ data/              # Data pipeline tests
â”‚   â”œâ”€â”€ training/          # Training tests
â”‚   â”œâ”€â”€ backend/           # API tests
â”‚   â””â”€â”€ integration/       # End-to-end tests
â”œâ”€â”€ docs/                  # Documentation
â”‚   â”œâ”€â”€ architecture.md    # System design
â”‚   â””â”€â”€ api.md             # API documentation
â””â”€â”€ scripts/               # Helper scripts
    â”œâ”€â”€ setup_env.sh       # Create conda environment
    â”œâ”€â”€ activate_env.sh    # Activate environment
    â”œâ”€â”€ clean_env.sh       # Remove environment
    â”œâ”€â”€ verify_setup.sh    # Verify project setup
    â””â”€â”€ init_git.sh        # Initialize git repository
```

## Technology Stack

- **Language**: Python 3.10
- **Model**: Mistral-7B (fine-tuned)
- **Backend**: FastAPI
- **Frontend**: Next.js
- **Training**: GCP (Google Cloud Platform)
- **Deployment**: GCP Cloud Run
- **Dataset**: Spider (Text-to-SQL)

## Environment Setup

```bash
# Create conda environment
./scripts/setup_env.sh

# Activate environment
source ./scripts/activate_env.sh

# Verify setup
./scripts/verify_setup.sh
```

## Data Download

The project uses the [Spider dataset](https://yale-lily.github.io/spider) - a large-scale text-to-SQL benchmark from Yale containing 10,181 questions across 200+ databases.

### Download Spider Dataset

```bash
# Download and extract Spider dataset
./scripts/download_spider.sh

# Verify the download
python scripts/verify_spider.py
```

### Expected Output

After successful download, `data/raw/spider/` will contain:

```
data/raw/spider/
â”œâ”€â”€ train_spider.json          # ~8,659 training examples
â”œâ”€â”€ train_others.json          # ~1,659 additional examples
â”œâ”€â”€ dev.json                   # ~1,034 validation examples
â”œâ”€â”€ tables.json                # Database schema definitions
â”œâ”€â”€ database/                  # 200+ SQLite databases
â”‚   â”œâ”€â”€ concert_singer/
â”‚   â”‚   â””â”€â”€ concert_singer.sqlite
â”‚   â”œâ”€â”€ pets_1/
â”‚   â”‚   â””â”€â”€ pets_1.sqlite
â”‚   â””â”€â”€ ...
â””â”€â”€ download_summary.txt       # Download verification
```

### Troubleshooting

**Download fails automatically:**
- Visit https://yale-lily.github.io/spider manually
- Download the Spider dataset zip file
- Place it in `data/raw/spider/spider.zip`
- Run `./scripts/download_spider.sh` again to extract

**JSON parsing errors:**
- Re-download the dataset (file may be corrupted)
- Check if extraction completed fully

**Reference:**
- [Spider Dataset Paper](https://arxiv.org/abs/1809.08887)
- [Spider Leaderboard](https://yale-lily.github.io/spider)

## Data Processing Pipeline

```
Spider Dataset (Yale NLP)
    â”‚
    â–¼
Module 1.1: Download
    â”‚   ./scripts/download_spider.sh
    â”‚   â†’ data/raw/spider/
    â”‚
    â–¼
Module 1.2: Parse Schemas
    â”‚   ./scripts/parse_schemas.sh
    â”‚   â†’ data/processed/schemas/
    â”‚   â†’ schema_index.json
    â”‚
    â–¼
Module 1.3: Format for Mistral
    â”‚   ./scripts/convert_to_mistral.sh
    â”‚   â†’ data/processed/train_mistral.jsonl
    â”‚   â†’ data/processed/dev_mistral.jsonl
    â”‚
    â–¼
Module 1.4: Convert to PostgreSQL  â† CURRENT
    â”‚   ./scripts/convert_to_postgres.sh
    â”‚   â†’ data/processed/train_postgres.jsonl
    â”‚   â†’ data/processed/dev_postgres.jsonl
    â”‚
    â–¼
Module 1.5: Create splits (next)
    â”‚
    â–¼
Ready for Fine-tuning
```

### Running the Pipeline

```bash
# Step 1: Download Spider dataset
./scripts/download_spider.sh

# Step 2: Parse and index schemas
./scripts/parse_schemas.sh

# Step 3: Convert to Mistral instruction format
./scripts/convert_to_mistral.sh

# Step 4: Convert SQLite to PostgreSQL syntax
./scripts/convert_to_postgres.sh

# Verify output
head -1 data/processed/train_postgres.jsonl | python -m json.tool
```

## License

MIT
