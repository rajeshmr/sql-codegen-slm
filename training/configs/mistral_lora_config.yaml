# Mistral-7B LoRA Fine-tuning Configuration
# SQL Code Generation for PostgreSQL
# Optimized for Google Colab Pro+

# Model Configuration
model:
  name: "mistralai/Mistral-7B-v0.1"
  max_seq_length: 2048
  torch_dtype: "float16"

# LoRA Configuration
lora:
  r: 16                          # LoRA rank - lower = fewer params, faster but less expressive
  lora_alpha: 32                 # LoRA scaling factor (alpha/r = 2 is standard)
  lora_dropout: 0.05             # Dropout for regularization
  target_modules:
    - "q_proj"                   # Query projection
    - "k_proj"                   # Key projection
    - "v_proj"                   # Value projection
    - "o_proj"                   # Output projection
    - "gate_proj"                # MLP gate
    - "up_proj"                  # MLP up projection
    - "down_proj"                # MLP down projection
  bias: "none"
  task_type: "CAUSAL_LM"

# Quantization Configuration (4-bit for memory efficiency)
quantization:
  load_in_4bit: true
  bnb_4bit_compute_dtype: "float16"
  bnb_4bit_use_double_quant: true
  bnb_4bit_quant_type: "nf4"     # NormalFloat4 - best for LLMs

# Training Configuration (optimized for Colab)
training:
  output_dir: "/content/models"  # Local during training, sync to GCS after
  num_train_epochs: 3
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  gradient_accumulation_steps: 4  # Effective batch size = 4 * 4 = 16
  gradient_checkpointing: true    # Save memory at cost of speed
  optim: "paged_adamw_32bit"      # Memory-efficient optimizer
  learning_rate: 0.0002           # 2e-4, standard for LoRA
  weight_decay: 0.001
  warmup_ratio: 0.03              # 3% of steps for warmup
  lr_scheduler_type: "cosine"
  max_grad_norm: 0.3
  fp16: false
  bf16: true                      # Better precision than fp16
  logging_steps: 10
  save_strategy: "steps"          # Changed to steps for Colab (handle disconnects)
  save_steps: 500                 # Save every 500 steps
  evaluation_strategy: "steps"    # Changed to steps
  eval_steps: 500                 # Eval every 500 steps
  save_total_limit: 2             # Only keep 2 checkpoints (save Drive space)
  load_best_model_at_end: true
  metric_for_best_model: "eval_loss"
  greater_is_better: false
  report_to: ["tensorboard"]      # Removed wandb for simplicity
  remove_unused_columns: false

# Data Configuration (Colab paths)
data:
  train_file: "/content/data/train_postgres.jsonl"
  val_file: "/content/data/val_postgres.jsonl"
  test_file: "/content/data/test_postgres.jsonl"
  text_column: "text"

# Logging Configuration
logging:
  project_name: "sql-codegen-slm"
  run_name: "mistral-7b-lora-sql-colab"
  log_dir: "/content/logs"
  tensorboard_dir: "/content/tensorboard"
