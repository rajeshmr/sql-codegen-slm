{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "A100"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL Codegen SLM - Full Production Training\n",
    "\n",
    "**Fine-tune Mistral-7B for PostgreSQL query generation**\n",
    "\n",
    "## Training Configuration\n",
    "- **Dataset**: 6,016 train + 332 val examples\n",
    "- **Epochs**: 3\n",
    "- **Estimated time**: 8-12 hours on A100\n",
    "- **Output**: `gs://sql-codegen-slm-data/models/mistral-sql-final`\n",
    "\n",
    "## Pre-validated ‚úÖ\n",
    "- GPU: A100-SXM4-40GB\n",
    "- Memory: 13GB / 42.5GB (safe)\n",
    "- Training pipeline verified\n",
    "- GCS sync working"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. GPU Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "print(f\"\\nPyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"GPU: {gpu_name}\")\n",
    "    print(f\"Memory: {gpu_mem:.1f} GB\")\n",
    "    \n",
    "    if 'A100' in gpu_name:\n",
    "        print(\"\\n‚úÖ A100 detected - optimal for training\")\n",
    "    elif gpu_mem >= 16:\n",
    "        print(\"\\n‚úÖ GPU has sufficient memory\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è GPU may have insufficient memory - consider A100\")\n",
    "else:\n",
    "    raise RuntimeError(\"‚ùå No GPU detected! Enable GPU in Runtime > Change runtime type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Try Colab secrets first, then environment variables\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    PROJECT_ID = userdata.get('GCP_PROJECT_ID')\n",
    "except:\n",
    "    PROJECT_ID = os.environ.get('GCP_PROJECT_ID', 'your-gcp-project-id')\n",
    "\n",
    "# Configuration\n",
    "BUCKET_NAME = os.environ.get('GCS_BUCKET', 'sql-codegen-slm-data')\n",
    "REPO_URL = 'https://github.com/rajeshmr/sql-codegen-slm.git'\n",
    "OUTPUT_DIR = '/content/models/mistral-sql-final'\n",
    "GCS_OUTPUT = f'gs://{BUCKET_NAME}/models/mistral-sql-final'\n",
    "\n",
    "# Set environment\n",
    "os.environ['GCP_PROJECT_ID'] = PROJECT_ID\n",
    "os.environ['GCS_BUCKET'] = BUCKET_NAME\n",
    "\n",
    "# Estimate completion time\n",
    "start_time = datetime.now()\n",
    "estimated_hours = 10  # Conservative estimate\n",
    "estimated_end = start_time + timedelta(hours=estimated_hours)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TRAINING CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Project ID: {PROJECT_ID}\")\n",
    "print(f\"GCS Bucket: {BUCKET_NAME}\")\n",
    "print(f\"Output: {GCS_OUTPUT}\")\n",
    "print(f\"\\nStart time: {start_time.strftime('%Y-%m-%d %H:%M')}\")\n",
    "print(f\"Estimated completion: {estimated_end.strftime('%Y-%m-%d %H:%M')} (~{estimated_hours}h)\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Authenticate GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "!gcloud config set project {PROJECT_ID}\n",
    "\n",
    "# Verify bucket access\n",
    "!gsutil ls gs://{BUCKET_NAME}/ | head -3\n",
    "print(f\"\\n‚úÖ GCS authenticated: gs://{BUCKET_NAME}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Clone Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists('sql-codegen-slm'):\n",
    "    !git clone {REPO_URL}\n",
    "    print(\"‚úÖ Repository cloned\")\n",
    "else:\n",
    "    print(\"Repository exists, pulling latest...\")\n",
    "    \n",
    "%cd sql-codegen-slm\n",
    "!git pull\n",
    "print(f\"\\n‚úÖ Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -r training/requirements.txt\n",
    "\n",
    "# Verify key packages\n",
    "import transformers\n",
    "import peft\n",
    "import bitsandbytes\n",
    "\n",
    "print(f\"transformers: {transformers.__version__}\")\n",
    "print(f\"peft: {peft.__version__}\")\n",
    "print(f\"bitsandbytes: {bitsandbytes.__version__}\")\n",
    "print(\"\\n‚úÖ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Download Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create directories\n",
    "os.makedirs('/content/data', exist_ok=True)\n",
    "os.makedirs('/content/models', exist_ok=True)\n",
    "os.makedirs('/content/logs', exist_ok=True)\n",
    "os.makedirs('/content/tensorboard', exist_ok=True)\n",
    "\n",
    "# Download data from GCS\n",
    "print(\"Downloading training data...\")\n",
    "!gsutil -m cp gs://{BUCKET_NAME}/data/train_postgres.jsonl /content/data/\n",
    "!gsutil -m cp gs://{BUCKET_NAME}/data/val_postgres.jsonl /content/data/\n",
    "\n",
    "# Verify\n",
    "!echo \"\\nDataset sizes:\"\n",
    "!wc -l /content/data/*.jsonl\n",
    "\n",
    "print(\"\\n‚úÖ Data downloaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Create Full Training Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "# Full production training configuration\n",
    "full_config = {\n",
    "    'model': {\n",
    "        'name': 'mistralai/Mistral-7B-v0.1',\n",
    "        'max_seq_length': 2048,\n",
    "    },\n",
    "    'lora': {\n",
    "        'r': 16,\n",
    "        'lora_alpha': 32,\n",
    "        'lora_dropout': 0.05,\n",
    "        'target_modules': [\n",
    "            'q_proj',\n",
    "            'k_proj', \n",
    "            'v_proj',\n",
    "            'o_proj',\n",
    "            'gate_proj',\n",
    "            'up_proj',\n",
    "            'down_proj'\n",
    "        ],\n",
    "        'bias': 'none',\n",
    "        'task_type': 'CAUSAL_LM',\n",
    "    },\n",
    "    'quantization': {\n",
    "        'load_in_4bit': True,\n",
    "        'bnb_4bit_compute_dtype': 'bfloat16',\n",
    "        'bnb_4bit_use_double_quant': True,\n",
    "        'bnb_4bit_quant_type': 'nf4',\n",
    "    },\n",
    "    'training': {\n",
    "        'output_dir': OUTPUT_DIR,\n",
    "        'num_train_epochs': 3,\n",
    "        'per_device_train_batch_size': 4,\n",
    "        'per_device_eval_batch_size': 4,\n",
    "        'gradient_accumulation_steps': 4,\n",
    "        'gradient_checkpointing': True,\n",
    "        'optim': 'paged_adamw_32bit',\n",
    "        'learning_rate': 2e-4,\n",
    "        'weight_decay': 0.001,\n",
    "        'warmup_ratio': 0.03,\n",
    "        'lr_scheduler_type': 'cosine',\n",
    "        'max_grad_norm': 0.3,\n",
    "        'fp16': False,\n",
    "        'bf16': True,\n",
    "        'logging_steps': 10,\n",
    "        'save_strategy': 'steps',\n",
    "        'save_steps': 500,\n",
    "        'eval_strategy': 'steps',\n",
    "        'eval_steps': 500,\n",
    "        'save_total_limit': 3,\n",
    "        'load_best_model_at_end': True,\n",
    "        'metric_for_best_model': 'eval_loss',\n",
    "        'greater_is_better': False,\n",
    "        'report_to': ['tensorboard'],\n",
    "        'remove_unused_columns': False,\n",
    "    },\n",
    "    'data': {\n",
    "        'train_file': '/content/data/train_postgres.jsonl',\n",
    "        'val_file': '/content/data/val_postgres.jsonl',\n",
    "        'max_samples_train': None,  # Use ALL data\n",
    "        'max_samples_val': None,    # Use ALL data\n",
    "    },\n",
    "    'logging': {\n",
    "        'log_dir': '/content/logs',\n",
    "        'tensorboard_dir': '/content/tensorboard',\n",
    "    },\n",
    "    'gcs': {\n",
    "        'bucket': BUCKET_NAME,\n",
    "        'sync_checkpoints': True,\n",
    "        'output_prefix': 'models/mistral-sql-final',\n",
    "    },\n",
    "}\n",
    "\n",
    "# Save config\n",
    "config_path = '/content/full_training_config.yaml'\n",
    "with open(config_path, 'w') as f:\n",
    "    yaml.dump(full_config, f, default_flow_style=False)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"FULL TRAINING CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Model: {full_config['model']['name']}\")\n",
    "print(f\"LoRA rank: {full_config['lora']['r']}\")\n",
    "print(f\"LoRA modules: {len(full_config['lora']['target_modules'])} (all projections)\")\n",
    "print(f\"Epochs: {full_config['training']['num_train_epochs']}\")\n",
    "print(f\"Batch size: {full_config['training']['per_device_train_batch_size']}\")\n",
    "print(f\"Gradient accumulation: {full_config['training']['gradient_accumulation_steps']}\")\n",
    "print(f\"Effective batch size: {full_config['training']['per_device_train_batch_size'] * full_config['training']['gradient_accumulation_steps']}\")\n",
    "print(f\"Learning rate: {full_config['training']['learning_rate']}\")\n",
    "print(f\"Checkpoint every: {full_config['training']['save_steps']} steps\")\n",
    "print(f\"\\nConfig saved: {config_path}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Pre-flight Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"PRE-FLIGHT CHECKS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "checks_passed = True\n",
    "\n",
    "# Check 1: GPU\n",
    "if torch.cuda.is_available():\n",
    "    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"‚úÖ GPU available: {torch.cuda.get_device_name(0)} ({gpu_mem:.0f}GB)\")\n",
    "else:\n",
    "    print(\"‚ùå No GPU available\")\n",
    "    checks_passed = False\n",
    "\n",
    "# Check 2: Training data\n",
    "train_file = '/content/data/train_postgres.jsonl'\n",
    "val_file = '/content/data/val_postgres.jsonl'\n",
    "if os.path.exists(train_file) and os.path.exists(val_file):\n",
    "    train_lines = sum(1 for _ in open(train_file))\n",
    "    val_lines = sum(1 for _ in open(val_file))\n",
    "    print(f\"‚úÖ Training data: {train_lines} train, {val_lines} val examples\")\n",
    "else:\n",
    "    print(\"‚ùå Training data not found\")\n",
    "    checks_passed = False\n",
    "\n",
    "# Check 3: Config file\n",
    "if os.path.exists('/content/full_training_config.yaml'):\n",
    "    print(\"‚úÖ Config file ready\")\n",
    "else:\n",
    "    print(\"‚ùå Config file not found\")\n",
    "    checks_passed = False\n",
    "\n",
    "# Check 4: Disk space\n",
    "import shutil\n",
    "total, used, free = shutil.disk_usage('/content')\n",
    "free_gb = free / (1024**3)\n",
    "if free_gb > 50:\n",
    "    print(f\"‚úÖ Disk space: {free_gb:.0f}GB free\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Low disk space: {free_gb:.0f}GB free\")\n",
    "\n",
    "# Check 5: Memory\n",
    "torch.cuda.empty_cache()\n",
    "allocated = torch.cuda.memory_allocated(0) / 1e9\n",
    "print(f\"‚úÖ GPU memory clear: {allocated:.2f}GB allocated\")\n",
    "\n",
    "# Check 6: GCS access\n",
    "import subprocess\n",
    "result = subprocess.run(['gsutil', 'ls', f'gs://{BUCKET_NAME}/'], capture_output=True, text=True)\n",
    "if result.returncode == 0:\n",
    "    print(f\"‚úÖ GCS bucket accessible\")\n",
    "else:\n",
    "    print(\"‚ùå GCS bucket not accessible\")\n",
    "    checks_passed = False\n",
    "\n",
    "print(\"=\"*60)\n",
    "if checks_passed:\n",
    "    print(\"üöÄ ALL CHECKS PASSED - READY FOR TRAINING\")\n",
    "else:\n",
    "    print(\"‚ùå SOME CHECKS FAILED - FIX BEFORE TRAINING\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Start TensorBoard (Optional)\n",
    "\n",
    "Run this cell to monitor training in real-time. You can also run it in a separate tab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir /content/tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. üöÄ START FULL TRAINING\n",
    "\n",
    "**This will take 8-12 hours on A100.**\n",
    "\n",
    "- Checkpoints saved every 500 steps to GCS\n",
    "- Training can be resumed if disconnected\n",
    "- Monitor progress in TensorBoard above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üöÄ STARTING FULL PRODUCTION TRAINING\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Estimated completion: ~8-12 hours\")\n",
    "print(f\"Output: {GCS_OUTPUT}\")\n",
    "print(\"=\"*60)\n",
    "print(\"\")\n",
    "\n",
    "!python -m training.train --config /content/full_training_config.yaml\n",
    "\n",
    "print(\"\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Training completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Sync Final Model to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Syncing final model to GCS...\")\n",
    "!gsutil -m rsync -r {OUTPUT_DIR} {GCS_OUTPUT}/\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ MODEL SAVED\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Location: {GCS_OUTPUT}/\")\n",
    "print(\"\\nFiles:\")\n",
    "!gsutil ls -l {GCS_OUTPUT}/ | tail -10\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 12. Test Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training.model_utils import load_model_and_tokenizer\n",
    "import yaml\n",
    "\n",
    "# Load config\n",
    "with open('/content/full_training_config.yaml') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Update to load from trained checkpoint\n",
    "config['model']['name'] = OUTPUT_DIR\n",
    "\n",
    "print(\"Loading trained model...\")\n",
    "model, tokenizer = load_model_and_tokenizer(config)\n",
    "\n",
    "# Test inference\n",
    "test_prompt = \"\"\"You are a PostgreSQL expert. Generate SQL for the following:\n",
    "\n",
    "Database: ecommerce\n",
    "\n",
    "Schema:\n",
    "CREATE TABLE customers (customer_id SERIAL PRIMARY KEY, name VARCHAR(100), email VARCHAR(100));\n",
    "CREATE TABLE orders (order_id SERIAL PRIMARY KEY, customer_id INTEGER REFERENCES customers(customer_id), total DECIMAL(10,2), created_at TIMESTAMP);\n",
    "\n",
    "Question: Find the top 5 customers by total order value\"\"\"\n",
    "\n",
    "inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=256,\n",
    "        temperature=0.1,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "    )\n",
    "\n",
    "generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"MODEL INFERENCE TEST\")\n",
    "print(\"=\"*60)\n",
    "print(generated)\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 13. Training Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import torch\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TRAINING COMPLETE - SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Completion time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"\")\n",
    "print(f\"Model location: {GCS_OUTPUT}/\")\n",
    "print(f\"TensorBoard logs: gs://{BUCKET_NAME}/logs/\")\n",
    "print(f\"\")\n",
    "print(\"Files saved:\")\n",
    "print(\"  - adapter_model.safetensors (LoRA weights)\")\n",
    "print(\"  - adapter_config.json\")\n",
    "print(\"  - tokenizer files\")\n",
    "print(f\"\")\n",
    "print(\"GPU Memory used:\")\n",
    "print(f\"  Max allocated: {torch.cuda.max_memory_allocated(0) / 1e9:.2f} GB\")\n",
    "print(f\"  Max reserved: {torch.cuda.max_memory_reserved(0) / 1e9:.2f} GB\")\n",
    "print(\"=\"*60)\n",
    "print(\"\")\n",
    "print(\"üéâ Training complete! Model ready for evaluation.\")\n",
    "print(\"\")\n",
    "print(\"Next steps:\")\n",
    "print(\"1. Download model: gsutil -m cp -r {GCS_OUTPUT}/ ./model/\")\n",
    "print(\"2. Run evaluation notebook\")\n",
    "print(\"3. Deploy to inference endpoint\")"
   ]
  }
 ]
}
