{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL Codegen SLM - Training Validation\n",
    "\n",
    "**Module 2.3: Training Validation & Smoke Testing**\n",
    "\n",
    "This notebook validates the complete training pipeline before running the full 8-12 hour training.\n",
    "\n",
    "## What Gets Validated\n",
    "- âœ… Data loading (20 examples)\n",
    "- âœ… Model initialization (4-bit + LoRA)\n",
    "- âœ… Training loop (20 steps)\n",
    "- âœ… Checkpoint saving to GCS\n",
    "- âœ… Checkpoint loading\n",
    "- âœ… Model inference (SQL generation)\n",
    "- âœ… GPU memory usage (<20GB)\n",
    "\n",
    "**Expected time: 10-15 minutes on A100**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Setup and GPU Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "print(f\"\\nPyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repo (use your GitHub token for private repo)\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# Get GitHub token\n",
    "GITHUB_TOKEN = getpass(\"Enter GitHub Personal Access Token: \")\n",
    "GITHUB_USERNAME = \"your-github-username\"  # Replace with your GitHub username\n",
    "\n",
    "# Clone with token\n",
    "if not os.path.exists('sql-codegen-slm'):\n",
    "    !git clone https://{GITHUB_USERNAME}:{GITHUB_TOKEN}@github.com/{GITHUB_USERNAME}/sql-codegen-slm.git\n",
    "else:\n",
    "    print(\"Repository already cloned\")\n",
    "\n",
    "%cd sql-codegen-slm\n",
    "!git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q -r training/requirements.txt\n",
    "print(\"\\nâœ… Dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Authenticate GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "\n",
    "# Set project - UPDATE THESE VALUES\n",
    "PROJECT_ID = \"your-gcp-project-id\"  # Replace with your GCP project ID\n",
    "BUCKET_NAME = \"sql-codegen-slm-data\"\n",
    "\n",
    "!gcloud config set project {PROJECT_ID}\n",
    "\n",
    "# Verify access\n",
    "!gsutil ls gs://{BUCKET_NAME}/ | head -5\n",
    "print(f\"\\nâœ… GCS bucket accessible: gs://{BUCKET_NAME}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Download Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create data directory\n",
    "os.makedirs('/content/data', exist_ok=True)\n",
    "\n",
    "# Download data from GCS\n",
    "!gsutil -m cp gs://{BUCKET_NAME}/data/train_postgres.jsonl /content/data/\n",
    "!gsutil -m cp gs://{BUCKET_NAME}/data/val_postgres.jsonl /content/data/\n",
    "\n",
    "# Check file sizes\n",
    "!wc -l /content/data/*.jsonl\n",
    "\n",
    "print(\"\\nâœ… Data downloaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create small test files (20 train, 10 val)\n",
    "import json\n",
    "\n",
    "# Take first 20 examples from train\n",
    "with open('/content/data/train_postgres.jsonl', 'r') as f:\n",
    "    train_lines = [next(f) for _ in range(20)]\n",
    "\n",
    "with open('/content/data/train_small.jsonl', 'w') as f:\n",
    "    f.writelines(train_lines)\n",
    "\n",
    "# Take first 10 examples from val\n",
    "with open('/content/data/val_postgres.jsonl', 'r') as f:\n",
    "    val_lines = [next(f) for _ in range(10)]\n",
    "    \n",
    "with open('/content/data/val_small.jsonl', 'w') as f:\n",
    "    f.writelines(val_lines)\n",
    "\n",
    "print(f\"Created test files:\")\n",
    "print(f\"  train_small.jsonl: {len(train_lines)} examples\")\n",
    "print(f\"  val_small.jsonl: {len(val_lines)} examples\")\n",
    "print(\"\\nâœ… Test data ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Test Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import sys\n",
    "sys.path.insert(0, '/content/sql-codegen-slm')\n",
    "\n",
    "from training.validation import test_data_loading\n",
    "\n",
    "# Load test config\n",
    "with open('training/configs/test_config.yaml') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Update paths for Colab\n",
    "config['data']['train_file'] = '/content/data/train_small.jsonl'\n",
    "config['data']['val_file'] = '/content/data/val_small.jsonl'\n",
    "\n",
    "# Run test\n",
    "success, details = test_data_loading(config)\n",
    "\n",
    "if success:\n",
    "    print(\"\\nâœ… Data loading test passed\")\n",
    "    print(f\"Train examples: {details['train_count']}\")\n",
    "    print(f\"Val examples: {details['val_count']}\")\n",
    "    print(f\"\\nSample format:\")\n",
    "    print(details['sample_format'][:500])\n",
    "else:\n",
    "    print(\"\\nâŒ Data loading test failed\")\n",
    "    print(details.get('error', 'Unknown error'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Test Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training.validation import test_model_initialization\n",
    "\n",
    "print(\"Loading Mistral-7B with 4-bit quantization...\")\n",
    "print(\"This may take 2-3 minutes...\\n\")\n",
    "\n",
    "success, model_info = test_model_initialization(config)\n",
    "\n",
    "if success:\n",
    "    print(\"\\nâœ… Model initialization test passed\")\n",
    "    print(f\"Device: {model_info['device']}\")\n",
    "    print(f\"Total params: {model_info['total_params']:,}\")\n",
    "    print(f\"Trainable params: {model_info['trainable_params']:,}\")\n",
    "    print(f\"Trainable %: {model_info['trainable_pct']:.2f}%\")\n",
    "    if 'gpu_memory_gb' in model_info:\n",
    "        print(f\"GPU memory: {model_info['gpu_memory_gb']:.2f} GB\")\n",
    "else:\n",
    "    print(\"\\nâŒ Model initialization failed\")\n",
    "    print(model_info.get('error', 'Unknown error'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Run Smoke Test (20 steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training.smoke_test import run_smoke_test\n",
    "import yaml\n",
    "\n",
    "print(\"ðŸ”¥ Running smoke test (20 training steps)...\")\n",
    "print(\"This will take 5-10 minutes\\n\")\n",
    "\n",
    "# Create test config with correct paths\n",
    "test_config = {\n",
    "    'model': {\n",
    "        'name': 'mistralai/Mistral-7B-v0.1',\n",
    "        'max_seq_length': 512,\n",
    "    },\n",
    "    'lora': {\n",
    "        'r': 8,\n",
    "        'lora_alpha': 16,\n",
    "        'lora_dropout': 0.05,\n",
    "        'target_modules': ['q_proj', 'v_proj'],\n",
    "        'bias': 'none',\n",
    "        'task_type': 'CAUSAL_LM',\n",
    "    },\n",
    "    'quantization': {\n",
    "        'load_in_4bit': True,\n",
    "        'bnb_4bit_compute_dtype': 'float16',\n",
    "        'bnb_4bit_use_double_quant': True,\n",
    "        'bnb_4bit_quant_type': 'nf4',\n",
    "    },\n",
    "    'training': {\n",
    "        'output_dir': '/content/test-models',\n",
    "        'per_device_train_batch_size': 2,\n",
    "        'gradient_accumulation_steps': 2,\n",
    "        'fp16': True,\n",
    "        'gradient_checkpointing': True,\n",
    "    },\n",
    "    'data': {\n",
    "        'train_file': '/content/data/train_small.jsonl',\n",
    "        'val_file': '/content/data/val_small.jsonl',\n",
    "        'max_samples_train': 20,\n",
    "        'max_samples_val': 10,\n",
    "    },\n",
    "}\n",
    "\n",
    "# Save temp config\n",
    "with open('/content/test_config.yaml', 'w') as f:\n",
    "    yaml.dump(test_config, f)\n",
    "\n",
    "# Run smoke test\n",
    "results = run_smoke_test('/content/test_config.yaml')\n",
    "\n",
    "if results['success']:\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"âœ… SMOKE TEST PASSED!\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Training loss: {results['train_loss']:.4f}\")\n",
    "    print(f\"Checkpoint saved: {results['checkpoint_path']}\")\n",
    "    print(f\"Model can generate SQL: {results['can_generate']}\")\n",
    "else:\n",
    "    print(f\"\\nâŒ SMOKE TEST FAILED at: {results['failure_point']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Test Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training.validation import test_inference\n",
    "from training.model_utils import load_model_and_tokenizer\n",
    "\n",
    "# Load model\n",
    "print(\"Loading model for inference test...\")\n",
    "model, tokenizer = load_model_and_tokenizer(test_config)\n",
    "\n",
    "test_prompt = \"\"\"You are a PostgreSQL expert. Generate SQL for the following:\n",
    "\n",
    "Database: ecommerce\n",
    "\n",
    "Schema:\n",
    "CREATE TABLE customers (customer_id SERIAL PRIMARY KEY, name VARCHAR(100));\n",
    "CREATE TABLE orders (order_id SERIAL PRIMARY KEY, customer_id INTEGER REFERENCES customers(customer_id));\n",
    "\n",
    "Question: Show all customers with their order counts\"\"\"\n",
    "\n",
    "success, generated_sql = test_inference(model, tokenizer, test_prompt)\n",
    "\n",
    "if success:\n",
    "    print(\"\\nâœ… Inference test passed\")\n",
    "    print(f\"\\nGenerated SQL:\\n{generated_sql}\")\n",
    "else:\n",
    "    print(\"\\nâŒ Inference test failed\")\n",
    "    print(generated_sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: Full Validation Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training.validation import validate_training_pipeline\n",
    "\n",
    "print(\"Running complete validation suite...\\n\")\n",
    "\n",
    "# Save config with correct paths\n",
    "with open('/content/validation_config.yaml', 'w') as f:\n",
    "    yaml.dump(test_config, f)\n",
    "\n",
    "report = validate_training_pipeline('/content/validation_config.yaml')\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"VALIDATION SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for test_name, result in report.items():\n",
    "    if test_name in ['all_passed', 'timestamp']:\n",
    "        continue\n",
    "    status = \"âœ…\" if result['passed'] else \"âŒ\"\n",
    "    print(f\"{status} {test_name}: {result['message']}\")\n",
    "\n",
    "if report.get('all_passed', False):\n",
    "    print(\"\\nðŸŽ‰ ALL TESTS PASSED - READY FOR FULL TRAINING\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸  SOME TESTS FAILED - FIX ISSUES BEFORE FULL TRAINING\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 9: GPU Memory Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(\"GPU Memory Usage:\")\n",
    "print(f\"Allocated: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n",
    "print(f\"Reserved: {torch.cuda.memory_reserved(0) / 1e9:.2f} GB\")\n",
    "print(f\"Max allocated: {torch.cuda.max_memory_allocated(0) / 1e9:.2f} GB\")\n",
    "\n",
    "total_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "allocated = torch.cuda.memory_allocated(0) / 1e9\n",
    "\n",
    "print(f\"\\nTotal GPU memory: {total_memory:.1f} GB\")\n",
    "print(f\"Usage: {100 * allocated / total_memory:.1f}%\")\n",
    "\n",
    "# Should be well under 40GB for A100\n",
    "if allocated < 20:\n",
    "    print(\"\\nâœ… Memory usage looks good for full training\")\n",
    "elif allocated < 30:\n",
    "    print(\"\\nâš ï¸  Moderate memory usage - should be OK for full training\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸  High memory usage - might need to reduce batch size\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 10: Cleanup Test Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Delete test checkpoints to save space\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# Clean local test artifacts\n",
    "test_dirs = ['/content/test-models', '/content/test-logs', '/content/test-tensorboard']\n",
    "for d in test_dirs:\n",
    "    if os.path.exists(d):\n",
    "        shutil.rmtree(d)\n",
    "        print(f\"Removed: {d}\")\n",
    "\n",
    "# Clean temp config files\n",
    "for f in ['/content/test_config.yaml', '/content/validation_config.yaml']:\n",
    "    if os.path.exists(f):\n",
    "        os.remove(f)\n",
    "\n",
    "# Clear GPU memory\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\nâœ… Test artifacts cleaned up\")\n",
    "print(f\"GPU memory after cleanup: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "If all validation tests passed:\n",
    "\n",
    "1. âœ… Your training pipeline is solid\n",
    "2. âœ… Ready for 8-12 hour full training\n",
    "3. âœ… Proceed to `notebooks/train_colab.ipynb`\n",
    "\n",
    "If any tests failed:\n",
    "\n",
    "1. Check error messages carefully\n",
    "2. Verify GCS authentication\n",
    "3. Check GPU allocation (need A100 or V100)\n",
    "4. Review logs for details\n",
    "5. Fix issues and re-run validation"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
