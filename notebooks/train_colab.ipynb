{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL Codegen SLM - Training Notebook\n",
    "\n",
    "Fine-tune Mistral-7B for PostgreSQL query generation using LoRA and 4-bit quantization.\n",
    "\n",
    "**Requirements:**\n",
    "- Google Colab Pro+ (for A100 GPU access)\n",
    "- GCP Project with Cloud Storage bucket\n",
    "- ~8-12 hours training time\n",
    "\n",
    "**Storage:** Google Cloud Storage (faster than Drive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Check GPU Allocation\n",
    "\n",
    "First, verify you have a GPU. Ideally A100 (40GB), V100 (16GB) is okay, T4 will be slower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"\\n‚úÖ GPU: {gpu_name} ({gpu_mem:.1f} GB)\")\n",
    "    \n",
    "    if \"A100\" in gpu_name:\n",
    "        print(\"üéâ Got A100 - optimal for training!\")\n",
    "    elif \"V100\" in gpu_name:\n",
    "        print(\"‚ö†Ô∏è Got V100 - good, but A100 is faster\")\n",
    "    elif \"T4\" in gpu_name:\n",
    "        print(\"‚ö†Ô∏è Got T4 - training will be slower (~2x)\")\n",
    "        print(\"üí° Tip: Disconnect and reconnect to try for A100\")\n",
    "else:\n",
    "    print(\"‚ùå No GPU! Go to Runtime > Change runtime type > GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configure GCP Project\n",
    "\n",
    "Set your GCP project ID and bucket name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ö†Ô∏è UPDATE THESE VALUES\n",
    "PROJECT_ID = \"your-gcp-project-id\"  # Your GCP project ID\n",
    "BUCKET_NAME = \"sql-codegen-slm-data\"  # Your GCS bucket name\n",
    "\n",
    "# Set environment variables\n",
    "import os\n",
    "os.environ[\"GCP_PROJECT_ID\"] = PROJECT_ID\n",
    "os.environ[\"GCS_BUCKET\"] = BUCKET_NAME\n",
    "\n",
    "print(f\"Project: {PROJECT_ID}\")\n",
    "print(f\"Bucket: gs://{BUCKET_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Authenticate with Google Cloud\n",
    "\n",
    "This will open a popup to authenticate with your Google account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "\n",
    "# Configure gcloud\n",
    "!gcloud config set project {PROJECT_ID}\n",
    "\n",
    "print(\"\\n‚úÖ Authenticated with Google Cloud\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Clone Repository & Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository (update with your GitHub username)\n",
    "!git clone https://github.com/YOUR_USERNAME/sql-codegen-slm.git\n",
    "%cd sql-codegen-slm\n",
    "\n",
    "# Install dependencies\n",
    "!pip install -q -r training/requirements.txt\n",
    "\n",
    "print(\"\\n‚úÖ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Setup GCS Bucket & Download Data\n",
    "\n",
    "This will:\n",
    "1. Create the GCS bucket if it doesn't exist\n",
    "2. Download training data from GCS to local storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bucket if needed\n",
    "!gsutil ls gs://{BUCKET_NAME} 2>/dev/null || gsutil mb -l us gs://{BUCKET_NAME}\n",
    "\n",
    "# Create local directories\n",
    "!mkdir -p /content/data /content/models /content/logs /content/tensorboard\n",
    "\n",
    "# Download data from GCS\n",
    "!gsutil -m cp gs://{BUCKET_NAME}/data/*.jsonl /content/data/ 2>/dev/null || echo \"Data not in GCS yet\"\n",
    "\n",
    "# Check what we have\n",
    "!ls -la /content/data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5b. Upload Data to GCS (if not already there)\n",
    "\n",
    "**Run this on your LOCAL machine first:**\n",
    "```bash\n",
    "# From your sql-codegen-slm directory\n",
    "gsutil -m cp data/processed/*.jsonl gs://YOUR_BUCKET_NAME/data/\n",
    "```\n",
    "\n",
    "Or upload directly from Colab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option: Upload files directly to Colab, then to GCS\n",
    "# Uncomment if you need to upload data\n",
    "\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()  # Select your JSONL files\n",
    "# !mv *.jsonl /content/data/\n",
    "# !gsutil -m cp /content/data/*.jsonl gs://{BUCKET_NAME}/data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Verify Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training.colab_setup import setup_colab_environment, estimate_training_time\n",
    "\n",
    "# Full environment check\n",
    "status = setup_colab_environment(PROJECT_ID, BUCKET_NAME)\n",
    "\n",
    "# Estimate training time\n",
    "print(\"\\n\")\n",
    "estimate_training_time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Start Training\n",
    "\n",
    "This will:\n",
    "- Load Mistral-7B with 4-bit quantization\n",
    "- Apply LoRA adapters\n",
    "- Train for 3 epochs\n",
    "- Save checkpoints every 500 steps\n",
    "\n",
    "**Estimated time:** 8-12 hours on A100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "!python -m training.train --config training/configs/mistral_lora_config.yaml\n",
    "\n",
    "# If training was interrupted, resume from checkpoint:\n",
    "# !python -m training.train --config training/configs/mistral_lora_config.yaml --resume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Sync Checkpoints to GCS\n",
    "\n",
    "Run this periodically to backup checkpoints to GCS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sync checkpoints to GCS\n",
    "!gsutil -m rsync -r /content/models gs://{BUCKET_NAME}/models/\n",
    "\n",
    "print(f\"\\n‚úÖ Checkpoints synced to gs://{BUCKET_NAME}/models/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Monitor Training with TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir /content/tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Test the Model\n",
    "\n",
    "After training completes, test the model with sample queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model_path = \"/content/models\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a sample query\n",
    "schema = \"\"\"\n",
    "CREATE TABLE customers (\n",
    "    id SERIAL PRIMARY KEY,\n",
    "    name VARCHAR(100),\n",
    "    email VARCHAR(100)\n",
    ");\n",
    "\n",
    "CREATE TABLE orders (\n",
    "    id SERIAL PRIMARY KEY,\n",
    "    customer_id INTEGER REFERENCES customers(id),\n",
    "    total DECIMAL(10,2),\n",
    "    created_at TIMESTAMP\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "question = \"Find customers who have placed more than 5 orders\"\n",
    "\n",
    "prompt = f\"\"\"[INST] Given the following PostgreSQL schema:\n",
    "\n",
    "{schema}\n",
    "\n",
    "Write a SQL query to: {question} [/INST]\"\"\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=256,\n",
    "        temperature=0.1,\n",
    "        do_sample=True,\n",
    "    )\n",
    "\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "sql = response.split(\"[/INST]\")[-1].strip()\n",
    "\n",
    "print(\"üìù Question:\", question)\n",
    "print(\"\\nüîç Generated SQL:\")\n",
    "print(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Final Sync to GCS\n",
    "\n",
    "Upload final model and logs to GCS for permanent storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sync everything to GCS\n",
    "!gsutil -m rsync -r /content/models gs://{BUCKET_NAME}/models/\n",
    "!gsutil -m rsync -r /content/logs gs://{BUCKET_NAME}/logs/\n",
    "!gsutil -m rsync -r /content/tensorboard gs://{BUCKET_NAME}/tensorboard/\n",
    "\n",
    "print(f\"\\n‚úÖ All files synced to gs://{BUCKET_NAME}/\")\n",
    "print(f\"\\nView in console: https://console.cloud.google.com/storage/browser/{BUCKET_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "### Session Disconnected?\n",
    "1. Reconnect to Colab\n",
    "2. Run cells 1-6 again (auth, setup)\n",
    "3. Download latest checkpoint: `!gsutil -m cp -r gs://{BUCKET_NAME}/models/* /content/models/`\n",
    "4. Resume training with `--resume` flag\n",
    "\n",
    "### Out of Memory?\n",
    "- Reduce batch size: `per_device_train_batch_size: 2`\n",
    "- Increase gradient accumulation: `gradient_accumulation_steps: 8`\n",
    "\n",
    "### GCS Permission Denied?\n",
    "- Re-run authentication cell\n",
    "- Check bucket permissions in GCP Console"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
