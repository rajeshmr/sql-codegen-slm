{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL Codegen SLM - Training Notebook\n",
    "\n",
    "Fine-tune Mistral-7B for PostgreSQL query generation using LoRA and 4-bit quantization.\n",
    "\n",
    "**Requirements:**\n",
    "- Google Colab Pro+ (for A100 GPU access)\n",
    "- GCP Project with Cloud Storage\n",
    "- ~8-12 hours training time\n",
    "\n",
    "**Data:** Already uploaded to `gs://sql-codegen-slm-data/data/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Check GPU Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"\\n‚úÖ GPU: {gpu_name} ({gpu_mem:.1f} GB)\")\n",
    "    \n",
    "    if \"A100\" in gpu_name:\n",
    "        print(\"üéâ Got A100 - optimal for training!\")\n",
    "    elif \"V100\" in gpu_name:\n",
    "        print(\"‚ö†Ô∏è V100 - good, but A100 is faster\")\n",
    "    elif \"T4\" in gpu_name:\n",
    "        print(\"‚ö†Ô∏è T4 - training will be slower. Consider reconnecting for A100.\")\n",
    "else:\n",
    "    print(\"‚ùå No GPU! Go to Runtime > Change runtime type > GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configure & Authenticate GCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GCP Configuration\n",
    "PROJECT_ID = \"your-gcp-project-id\"\n",
    "BUCKET_NAME = \"sql-codegen-slm-data\"\n",
    "\n",
    "import os\n",
    "os.environ[\"GCP_PROJECT_ID\"] = PROJECT_ID\n",
    "os.environ[\"GCS_BUCKET\"] = BUCKET_NAME\n",
    "\n",
    "# Authenticate\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "\n",
    "!gcloud config set project {PROJECT_ID}\n",
    "print(f\"\\n‚úÖ Authenticated with project: {PROJECT_ID}\")\n",
    "print(f\"   Bucket: gs://{BUCKET_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Clone Repository & Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists('sql-codegen-slm'):\n",
    "    !git clone https://github.com/rajesh-manikka/sql-codegen-slm.git\n",
    "%cd sql-codegen-slm\n",
    "\n",
    "# Install dependencies\n",
    "!pip install -q -r training/requirements.txt\n",
    "\n",
    "print(\"\\n‚úÖ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Download Data from GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create local directories\n",
    "!mkdir -p /content/data /content/models /content/logs /content/tensorboard\n",
    "\n",
    "# Download data from GCS\n",
    "!gsutil -m cp gs://{BUCKET_NAME}/data/*.jsonl /content/data/\n",
    "\n",
    "# Verify\n",
    "print(\"\\nüìä Dataset:\")\n",
    "!wc -l /content/data/*.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Verify Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training.colab_setup import check_gpu, estimate_training_time\n",
    "\n",
    "# Check GPU\n",
    "gpu_info = check_gpu()\n",
    "\n",
    "# Estimate training time\n",
    "print(\"\\n\")\n",
    "estimate_training_time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Start Training\n",
    "\n",
    "**Estimated time:** 8-12 hours on A100\n",
    "\n",
    "Checkpoints save every 500 steps to `/content/models/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "!python -m training.train --config training/configs/mistral_lora_config.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If training was interrupted, resume from checkpoint:\n",
    "# !python -m training.train --config training/configs/mistral_lora_config.yaml --resume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Sync Checkpoints to GCS\n",
    "\n",
    "Run periodically to backup checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil -m rsync -r /content/models gs://{BUCKET_NAME}/models/\n",
    "print(f\"\\n‚úÖ Synced to gs://{BUCKET_NAME}/models/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Monitor with TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir /content/tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model_path = \"/content/models\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "print(\"‚úÖ Model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = \"\"\"\n",
    "CREATE TABLE customers (id SERIAL PRIMARY KEY, name VARCHAR(100), email VARCHAR(100));\n",
    "CREATE TABLE orders (id SERIAL PRIMARY KEY, customer_id INTEGER REFERENCES customers(id), total DECIMAL(10,2), created_at TIMESTAMP);\n",
    "\"\"\"\n",
    "\n",
    "question = \"Find customers who have placed more than 5 orders\"\n",
    "\n",
    "prompt = f\"[INST] Given the PostgreSQL schema:\\n{schema}\\nWrite SQL to: {question} [/INST]\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, max_new_tokens=256, temperature=0.1, do_sample=True)\n",
    "\n",
    "sql = tokenizer.decode(outputs[0], skip_special_tokens=True).split(\"[/INST]\")[-1].strip()\n",
    "print(f\"üìù Question: {question}\\n\\nüîç SQL:\\n{sql}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Final Sync to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil -m rsync -r /content/models gs://{BUCKET_NAME}/models/\n",
    "!gsutil -m rsync -r /content/tensorboard gs://{BUCKET_NAME}/tensorboard/\n",
    "\n",
    "print(f\"\\n‚úÖ All files synced to gs://{BUCKET_NAME}/\")\n",
    "print(f\"View: https://console.cloud.google.com/storage/browser/{BUCKET_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Troubleshooting\n",
    "\n",
    "**Session disconnected?**\n",
    "1. Reconnect, run cells 1-4\n",
    "2. Download checkpoint: `!gsutil -m cp -r gs://{BUCKET_NAME}/models/* /content/models/`\n",
    "3. Resume: `!python -m training.train --config training/configs/mistral_lora_config.yaml --resume`\n",
    "\n",
    "**Out of memory?** Reduce batch size in config to 2, increase gradient accumulation to 8."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {"gpuType": "A100", "provenance": []},
  "kernelspec": {"display_name": "Python 3", "name": "python3"},
  "language_info": {"name": "python", "version": "3.10.0"}
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
